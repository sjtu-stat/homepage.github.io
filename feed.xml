<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://lcns-sjtu.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://lcns-sjtu.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-12-18T12:30:56+00:00</updated><id>https://lcns-sjtu.github.io/feed.xml</id><title type="html">Lab of Computational Neuroscience</title><subtitle>Computational Neuroscience + Applied Mathematics. Research group headed by Douglas Zhou, PhD in the  Institute of Natural Science, Shanghai Jiao Tong University.
</subtitle><entry><title type="html">The role of population structure in computations through neural dynamics</title><link href="https://lcns-sjtu.github.io/blog/2022/KaiChen1/" rel="alternate" type="text/html" title="The role of population structure in computations through neural dynamics" /><published>2022-02-23T19:00:00+00:00</published><updated>2022-02-23T19:00:00+00:00</updated><id>https://lcns-sjtu.github.io/blog/2022/KaiChen1</id><content type="html" xml:base="https://lcns-sjtu.github.io/blog/2022/KaiChen1/"><![CDATA[<p>It has been widely recorded that the population neuronal activities pocess the low dimensional manifold in multiple brain regions, especially PFC. The origin of low dimensional dynamics and the relation between the dynamical properties and the network structures remain open questions. One of the potential solutions is that low dimensional dynamics is generated by low-rank network architectures. This work trained low-rank recurrent neural networks to perform 5 distinct cognitive tasks respectively, and theoretically analyzed the network dynamics performing computation for each task. Their work showed that very few ranks (1-2) of network structure are actually required to well perform those cognitive tasks. For those tasks with flexible input-target mapping, multiple cell-types (sub-populations) are necessary to perform tasks. Overall, their theory of low-rank RNN can extract the effective latent dynamics for computation, and furthermore provide a framework to networks with multitasking ability.</p>

<div class="row">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-02-23-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-02-23-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-02-23-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2022-02-23.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>]]></content><author><name>Kai Chen</name></author><category term="RNN" /><category term="low-rank-network" /><category term="dynamical-system" /><category term="computation" /><category term="global-structure" /><category term="gating-mechanism" /><summary type="html"><![CDATA[It has been widely recorded that the population neuronal activities pocess the low dimensional manifold in multiple brain regions, especially PFC. The origin of low dimensional dynamics and the relation between the dynamical properties and the network structures remain open questions. One of the potential solutions is that low dimensional dynamics is generated by low-rank network architectures. This work trained low-rank recurrent neural networks to perform 5 distinct cognitive tasks respectively, and theoretically analyzed the network dynamics performing computation for each task. Their work showed that very few ranks (1-2) of network structure are actually required to well perform those cognitive tasks. For those tasks with flexible input-target mapping, multiple cell-types (sub-populations) are necessary to perform tasks. Overall, their theory of low-rank RNN can extract the effective latent dynamics for computation, and furthermore provide a framework to networks with multitasking ability.]]></summary></entry><entry><title type="html">Context-dependent computation by recurrent dynamics in prefrontal cortex</title><link href="https://lcns-sjtu.github.io/blog/2021/KaiChen4/" rel="alternate" type="text/html" title="Context-dependent computation by recurrent dynamics in prefrontal cortex" /><published>2021-12-13T19:00:00+00:00</published><updated>2021-12-13T19:00:00+00:00</updated><id>https://lcns-sjtu.github.io/blog/2021/KaiChen4</id><content type="html" xml:base="https://lcns-sjtu.github.io/blog/2021/KaiChen4/"><![CDATA[<p>Mante et. al. showed how neurons with complex response coordinate together to do computations of selective integrations in monkey PFC. They trained an siRNN to model the psychophysical behavior of monkeys. By analyzing the modeled siRNN using theory of linear dynamical system, the response of siRNN fits almost perfectly with monkey data in the population level. Furthermore, siRNN produced a novel mechanism to unify selection and integration in a single circuit in terms of line attractor and selection vector.</p>]]></content><author><name>Kai Chen</name></author><category term="RNN" /><category term="fixed-point-analysis" /><category term="dynamical-system" /><category term="computation" /><summary type="html"><![CDATA[Mante et. al. showed how neurons with complex response coordinate together to do computations of selective integrations in monkey PFC. They trained an siRNN to model the psychophysical behavior of monkeys. By analyzing the modeled siRNN using theory of linear dynamical system, the response of siRNN fits almost perfectly with monkey data in the population level. Furthermore, siRNN produced a novel mechanism to unify selection and integration in a single circuit in terms of line attractor and selection vector.]]></summary></entry><entry><title type="html">Learning function from structure in neuromorphic networks</title><link href="https://lcns-sjtu.github.io/blog/2021/KaiChen3/" rel="alternate" type="text/html" title="Learning function from structure in neuromorphic networks" /><published>2021-09-08T19:00:00+00:00</published><updated>2021-09-08T19:00:00+00:00</updated><id>https://lcns-sjtu.github.io/blog/2021/KaiChen3</id><content type="html" xml:base="https://lcns-sjtu.github.io/blog/2021/KaiChen3/"><![CDATA[<p>Human brain can perform various of cognitive tasks and is able to flexibly learn new tasks without interfering other tasks. Whether and how the learning and computing capability is inherited from the brain connectomes remains unknown. This work tried to link the learning function and brain connectome in the framework of reservoir computing. They showed that the brain connectome outperform random network at crtical dynamical regime. Futhermore, they found that functional parcellation helps regulate the information flow which might facilitate the cognitive computation in brain</p>]]></content><author><name>Kai Chen</name></author><category term="RNN" /><category term="reservoir-computing" /><category term="ESN" /><category term="dynamical-system" /><category term="computation" /><category term="memory-capacity" /><summary type="html"><![CDATA[Human brain can perform various of cognitive tasks and is able to flexibly learn new tasks without interfering other tasks. Whether and how the learning and computing capability is inherited from the brain connectomes remains unknown. This work tried to link the learning function and brain connectome in the framework of reservoir computing. They showed that the brain connectome outperform random network at crtical dynamical regime. Futhermore, they found that functional parcellation helps regulate the information flow which might facilitate the cognitive computation in brain]]></summary></entry><entry><title type="html">One Step Back, Two Steps Forward: Interference and Learning in Recurrent Neural Networks</title><link href="https://lcns-sjtu.github.io/blog/2021/KaiChen2/" rel="alternate" type="text/html" title="One Step Back, Two Steps Forward: Interference and Learning in Recurrent Neural Networks" /><published>2021-05-20T19:00:00+00:00</published><updated>2021-05-20T19:00:00+00:00</updated><id>https://lcns-sjtu.github.io/blog/2021/KaiChen2</id><content type="html" xml:base="https://lcns-sjtu.github.io/blog/2021/KaiChen2/"><![CDATA[<p>Catastrophic forgetting is a key issue in continual learning paradigm. Training algorithms, like FORCE, seem to be able to bypass this to some extent. Chen and Barak applied fixed point analysis to explicitly show the change of fixed point structure of networks during training in continual learning scenario. Their work provide intuitions about how learning algorithm and the order of task sequence affect the training in continual learning.</p>

<ul>
  <li>FORCE: slower convergence in one trial, faster convergence in multi trials.</li>
  <li>FORCE: less biological plausible, more powerful in sequential learning.</li>
  <li>LMS (least mean square): smaller learning rate, less interference.</li>
</ul>]]></content><author><name>Kai Chen</name></author><category term="RNN" /><category term="fixed-point-analysis" /><category term="dynamical-system" /><category term="computation" /><summary type="html"><![CDATA[Catastrophic forgetting is a key issue in continual learning paradigm. Training algorithms, like FORCE, seem to be able to bypass this to some extent. Chen and Barak applied fixed point analysis to explicitly show the change of fixed point structure of networks during training in continual learning scenario. Their work provide intuitions about how learning algorithm and the order of task sequence affect the training in continual learning.]]></summary></entry><entry><title type="html">Universality and individuality in neural dynamics across large populations of recurrent networks</title><link href="https://lcns-sjtu.github.io/blog/2021/KaiChen1/" rel="alternate" type="text/html" title="Universality and individuality in neural dynamics across large populations of recurrent networks" /><published>2021-04-21T19:00:00+00:00</published><updated>2021-04-21T19:00:00+00:00</updated><id>https://lcns-sjtu.github.io/blog/2021/KaiChen1</id><content type="html" xml:base="https://lcns-sjtu.github.io/blog/2021/KaiChen1/"><![CDATA[<p>Multi-solution is a prominant feature of ANNs (DNNs/RNNs) when training to perform certain tasks. Is there any common feature between different solutions remains an open questions. This works found that the topology of fixed points of trained network is the universally shared between different network architectures and realizations when those networks are trained for the same task. Further, they demonstrated the topological structure of fixed points of networks indeed interprets computation mechanism of trained networks.</p>

<ul>
  <li>
    <p>Networks with different architectures and nonlinearities do differ in the sense of conventional representational similarity analysis (RSA).</p>
  </li>
  <li>
    <p>Fixed point topology can be invariant across networks performing same tasks.</p>
  </li>
  <li>
    <p>Linearized dynamics showed the computational ability in the dynamical regime around the fixed point points, which further proved the common dynamical feature across different network realizations.</p>
  </li>
  <li>
    <p>This RSA protocol on fixed point topology might be useful to quantify the similarity between RNN and BNNs (biological neuronal networks). (Also it is the critical issue when we use DL to study neurophysiological data.)</p>
  </li>
  <li>
    <p>Understanding fixed point topology and linear behavior around “generalized fixed points” can be a good perspective to study the computation through dynamics in RNNs, with related topics as memory capacity, learning dynamics, online learning (continual learning), and etc.</p>
  </li>
</ul>]]></content><author><name>Kai Chen</name></author><category term="RNN" /><category term="fixed-point-analysis" /><category term="dynamical-system" /><category term="computation" /><summary type="html"><![CDATA[Multi-solution is a prominant feature of ANNs (DNNs/RNNs) when training to perform certain tasks. Is there any common feature between different solutions remains an open questions. This works found that the topology of fixed points of trained network is the universally shared between different network architectures and realizations when those networks are trained for the same task. Further, they demonstrated the topological structure of fixed points of networks indeed interprets computation mechanism of trained networks.]]></summary></entry><entry><title type="html">Generating Coherent Patterns of Activity from Chaotic Neural Networks</title><link href="https://lcns-sjtu.github.io/blog/2020/KaiChen1/" rel="alternate" type="text/html" title="Generating Coherent Patterns of Activity from Chaotic Neural Networks" /><published>2020-12-09T19:00:00+00:00</published><updated>2020-12-09T19:00:00+00:00</updated><id>https://lcns-sjtu.github.io/blog/2020/KaiChen1</id><content type="html" xml:base="https://lcns-sjtu.github.io/blog/2020/KaiChen1/"><![CDATA[<p>Sussillo and Abbott improved the echo-state network (ESN) with FORCE algorithm, which is more robust again noise and perturbations. They also provided the analysis of train dynamics and reverse-engineering of network dynamics for reservoir network generating coherent patterns.</p>

<ul>
  <li>
    <p>Due to the chaotic nature, conventional BP fails in training RNN. Reservoir computing, using the echo-state property of RNNs, trains the output weight without manipulating the random reservoir current connectivity, and is capable to performing different tasks.</p>
  </li>
  <li>
    <p>Compared with ESN (Jaeger 2004), FORCE algorithm is more robust against perturbations, and is more capable of multi-task continual learning.</p>
  </li>
  <li>
    <p>The training dynamics and property of well-trained network were investigated.</p>

    <ul>
      <li>
        <p>During training, weights along eigenvectors with larger eigenvalues converge first, and the network encode task ability with multiple eigen-modes.</p>
      </li>
      <li>
        <p>Well trained networks have fixed projections along first few eigenvectors and accompanied with flexible projections alone rest of eigenvectors.</p>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>Kai Chen</name></author><category term="RNN" /><category term="reservoir-computing" /><category term="FORCE" /><category term="ESN" /><summary type="html"><![CDATA[Sussillo and Abbott improved the echo-state network (ESN) with FORCE algorithm, which is more robust again noise and perturbations. They also provided the analysis of train dynamics and reverse-engineering of network dynamics for reservoir network generating coherent patterns.]]></summary></entry></feed>